# -*- coding: utf-8 -*-
"""segnet_ii.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cyCqnWtSLMa7QcOGTdnS0h4IPm1R81MP
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Libraries
from __future__ import print_function
import shutil
import os
import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models
import torch.nn as nn
import torch.optim as optim
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
# %matplotlib inline
import math
from collections import OrderedDict
import torch.nn.functional as F
import pprint
import torch.utils.data as data 
from google.colab import drive
drive.mount("/content/drive", force_remount=True)
import sys
sys.path.append('/content/drive/My Drive')
import utils
import random

class CamVid(data.Dataset):
    """CamVid dataset loader where the dataset is arranged as in
    https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid.
    Keyword arguments:
    - root_dir (``string``): Root directory path.
    - mode (``string``): The type of dataset: 'train' for training set, 'val'
    for validation set, and 'test' for test set.
    - transform (``callable``, optional): A function/transform that  takes in
    an PIL image and returns a transformed version. Default: None.
    - label_transform (``callable``, optional): A function/transform that takes
    in the target and transforms it. Default: None.
    - loader (``callable``, optional): A function to load an image given its
    path. By default ``default_loader`` is used.
    """
    # Training dataset root folders
    train_folder = 'train'
    train_lbl_folder = 'trainannot'

    # Validation dataset root folders
    val_folder = 'val'
    val_lbl_folder = 'valannot'

    # Test dataset root folders
    test_folder = 'test'
    test_lbl_folder = 'testannot'

    # Images extension
    img_extension = '.png'

    def __init__(self,
           root_dir,
           mode='train',
           transform=None,
           label_transform=None,
           loader=utils.pil_loader):
      
        self.root_dir = root_dir
        self.mode = mode
        self.transform = transform
        self.label_transform = label_transform
        self.loader = loader

        if self.mode.lower() == 'train':
            # Get the training data and labels filepaths
            self.train_data = utils.get_files(
                os.path.join(root_dir, self.train_folder),
                extension_filter=self.img_extension)

            self.train_labels = utils.get_files(
                os.path.join(root_dir, self.train_lbl_folder),
                extension_filter=self.img_extension)
        elif self.mode.lower() == 'val':
            # Get the validation data and labels filepaths
            self.val_data = utils.get_files(
                os.path.join(root_dir, self.val_folder),
                extension_filter=self.img_extension)

            self.val_labels = utils.get_files(
                os.path.join(root_dir, self.val_lbl_folder),
                extension_filter=self.img_extension)
        elif self.mode.lower() == 'test':
            # Get the test data and labels filepaths
            self.test_data = utils.get_files(
                os.path.join(root_dir, self.test_folder),
                extension_filter=self.img_extension)

            self.test_labels = utils.get_files(
                os.path.join(root_dir, self.test_lbl_folder),
                extension_filter=self.img_extension)
        else:
            raise RuntimeError("Unexpected dataset mode. "
                               "Supported modes are: train, val and test")
    
    def __getitem__(self, index):
        """
        Args:
        - index (``int``): index of the item in the dataset
        Returns:
        A tuple of ``PIL.Image`` (image, label) where label is the ground-truth
        of the image.
        """
        if self.mode.lower() == 'train':
            data_path, label_path = self.train_data[index], self.train_labels[
                index]
        elif self.mode.lower() == 'val':
            data_path, label_path = self.val_data[index], self.val_labels[
                index]
        elif self.mode.lower() == 'test':
            data_path, label_path = self.test_data[index], self.test_labels[
                index]
        else:
            raise RuntimeError("Unexpected dataset mode. "
                               "Supported modes are: train, val and test")

        img, label = self.loader(data_path, label_path)
        if self.transform is not None:
            img = self.transform(img)
            
        if self.label_transform is not None:
            label = self.label_transform(label)
        label = label.permute(1,2,0)*255
        label_np = label.numpy()
        label = rgb_to_label(label_np, colormap=color_encoding)
        label = torch.LongTensor(label)
        return img, label

    def __len__(self):
        """Returns the length of the dataset."""
        if self.mode.lower() == 'train':
            return len(self.train_data)
        elif self.mode.lower() == 'val':
            return len(self.val_data)
        elif self.mode.lower() == 'test':
            return len(self.test_data)
        else:
            raise RuntimeError("Unexpected dataset mode. "
                               "Supported modes are: train, val and test")

# Default encoding for pixel value, class name, and class color
class_name = {0: 'sky',
      1: 'building',
      2: 'pole',
      3: 'road_marking',
      4: 'road',
      5: 'pavement',
      6: 'tree',
      7: 'sign_symbol',
      8: 'fence',
      9: 'car',
      10: 'pedestrian',
      11: 'bicyclist',
      12: 'unlabeled'}


color_encoding = {0: (128, 128, 128),
      1: (128, 0, 0),
      2: (192, 192, 128),
      3: (255, 69, 0),
      4: (128, 64, 128),
      5: (60, 40, 222),
      6: (128, 128, 0),
      7: (192, 128, 128),
      8: (64, 64, 128),
      9: (64, 0, 128),
      10: (64, 64, 0),
      11: (0, 128, 192),
      12: (0, 0, 0)}

def rgb_to_label(rgb_image, colormap = color_encoding):
    '''Function to one hot encode RGB mask labels
        Inputs: 
            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)
            colormap - dictionary of color to label id
        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)
    '''
    num_classes = len(colormap)
    shape = rgb_image.shape[:2]
    encoded_image = np.zeros(shape,dtype=np.int8)
    for i, cls in enumerate(colormap):
        for x in range(encoded_image.shape[0]):
            for y in range (encoded_image.shape[1]):
                if(np.all(rgb_image[x][y] == colormap[i])):
                    encoded_image[x][y] = i
    return encoded_image


def label_to_rgb(label, colormap = color_encoding):
    '''Function to decode encoded mask labels
        Inputs: 
            onehot - one hot encoded image matrix (height x width x num_classes)
            colormap - dictionary of color to label id
        Output: Decoded RGB image (height x width x 3) 
    '''
    output = np.zeros(label.shape[:2]+(3,))
    for k in colormap.keys():
        output[label==k] = colormap[k]
    return np.uint8(output)

# functions of Maddern's method and Alvarez's mathod
def invariant_Maddern(img,alpha):
  ii_image = (0.5 + torch.log(img[1, :, :]) - alpha * torch.log(img[2, :, :]) - (1 - alpha) * torch.log(img[0, :, :]) )
  return ii_image

def log_approx(x):
  alpha = 5000
  log_x = alpha*(pow(x,1/alpha)-1)
  return log_x

def invariant_Alvarez(img,theta):
  ii_image = 0.5 + math.cos(theta)*log_approx(img[0, :, :]/(img[2, :, :])+1e-10) + math.sin(theta)*log_approx(img[1, :, :]/(img[2, :, :])+1e-10)
  return ii_image

# Specify transforms using torchvision.transforms as transforms
# Process images using Maddern's method
# The one-channel gray image is transformed to have three same channels
res_ratio = 0.1 # set image size
alpha = 0.48

transformationsimg_Mad = transforms.Compose([
    transforms.Resize((int(res_ratio*720),int(res_ratio*960)),interpolation=Image.NEAREST), #set resolution with nearest neighbor interpolation
    transforms.ToTensor(),# Normalize data to be of values [0-1]
    transforms.Lambda(lambda img: invariant_Maddern(img,alpha)),
    transforms.Lambda(lambda img: img.repeat(3,1,1))
])

train = CamVid('/content/drive/My Drive/','train',transformationsimg_Mad, transformationslabel)
val = CamVid('/content/drive/My Drive/','val',transformationsimg_Mad, transformationslabel)
test = CamVid('/content/drive/My Drive/','test',transformationsimg_Mad, transformationslabel)

train_dataloader = data.DataLoader(train, batch_size= 4, shuffle = True, num_workers=4)
val_dataloader = data.DataLoader(val, batch_size= 4, shuffle = True, num_workers=4)
test_dataloader = data.DataLoader(test, batch_size= 4, shuffle = True, num_workers=4)

for index,[img,label] in enumerate(train_dataloader):
    print('image size:',img.size())
    print('label size:',label.size())
    mask_encoded = [label_to_rgb(label[x,:,:].numpy(), color_encoding) for x in range(label.shape[0])]
    for i in range(0,4):
        plt.figure()
        plt.subplot(1,2,1)
        plt.imshow(img[i].permute(1,2,0))
        plt.subplot(1,2,2)
        plt.imshow(mask_encoded[i])
    break

# Specify transforms using torchvision.transforms as transforms
# Alvarez's method
res_ratio = 0.1 # set image size
theta = 135

transformationsimg_Alvarez = transforms.Compose([
    transforms.Resize((int(res_ratio*720),int(res_ratio*960)),interpolation=Image.NEAREST), #set resolution with nearest neighbor interpolation
    transforms.ToTensor(),# Normalize data to be of values [0-1]
    transforms.Lambda(lambda img: invariant_Alvarez(img,theta)),
    transforms.Lambda(lambda img: img.repeat(3,1,1)), 
])

transformationslabel = transforms.Compose([
    transforms.Resize((int(res_ratio*720),int(res_ratio*960)),interpolation=Image.NEAREST), #set resolution with nearest neighbor interpolation
    transforms.ToTensor() # Normalize data to be of values [0-1]
])

train = CamVid('/content/drive/My Drive/','train',transformationsimg_Alvarez, transformationslabel)
val = CamVid('/content/drive/My Drive/','val',transformationsimg_Alvarez, transformationslabel)
test = CamVid('/content/drive/My Drive/','test',transformationsimg_Alvarez, transformationslabel)

train_dataloader = data.DataLoader(train, batch_size= 4, shuffle = True, num_workers=4)
val_dataloader = data.DataLoader(val, batch_size= 4, shuffle = True, num_workers=4)
test_dataloader = data.DataLoader(test, batch_size= 4, shuffle = True, num_workers=4)

for index,[img,label] in enumerate(train_dataloader):
    print(img.size())
    print(label.size())
    print(img.type())
    mask_encoded = [label_to_rgb(label[x,:,:].numpy(), color_encoding) for x in range(label.shape[0])]
    for i in range(0,4):
        plt.figure()
        plt.subplot(1,2,1)
        plt.imshow(img[i].permute(1,2,0))
        plt.subplot(1,2,2)
        plt.imshow(mask_encoded[i])
    break